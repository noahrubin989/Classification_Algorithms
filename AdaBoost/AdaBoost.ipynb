{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b004a8a6",
   "metadata": {},
   "source": [
    "# **Adaptive Boosting (AdaBoost)**\n",
    "\n",
    "Noah Rubin\n",
    "\n",
    "Self Study - July 2021\n",
    "\n",
    "---\n",
    "\n",
    "### Main Ideas:\n",
    "\n",
    "* AdaBoost is a meta-algorithm, meaning it [\"learns from the output of learning algorithms and make(s) a prediction given predictions made by other models.\"](https://machinelearningmastery.com/meta-learning-in-machine-learning/)\n",
    "* The idea of AdaBoost is \"to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data.\" - [Sklearn Documentation](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)\n",
    "* These tiny decision trees are sometimes called [decision stumps](https://en.wikipedia.org/wiki/Decision_stump#:~:text=A%20decision%20stump%20is%20a,just%20a%20single%20input%20feature.)\n",
    "* The modifications of the data at each boosting iteration involves applying weights $w_i$ for $i = 1...N$ to each of the observations in the training set\n",
    "* The learning process starts by setting each $w_i = \\frac{1}{N}$ for $i = 1...N$  to represent the idea that each sample initially receives the same weighting when fitting a weak learner on the original training dataset\n",
    "* These weights of course change with each iteration of the algorithm, where each sample's weight changes before the algorithm is reapplied to the data with modified weights. Observations with incorrect predictions have their weights increased while weights are decreased for samples predicted that output correct predictions\n",
    "* So as the number of iterations increases, samples that are often misclassified (difficult to predict) receive higher and higher weighting\n",
    "* This ensures that each subsequent weak learner concentrates more on samples predicted incorrectly by the previous decision stump in the sequence\n",
    "* Ultimately, with AdaBoost, while each 'weak learner' only does an ok job at classifying the samples, the final model with all the all the weak learners combined will converge to an overall strong learner\n",
    "* This differs from a random forest classifier since all individual learners in a random forest are built independently (and are not built through considering the mistakes from the previously built tree).\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Details:\n",
    "\n",
    "Suppose we have a training dataset $\\{\\vec{x}_i, y_i\\}_{i=1}^N$ with $N$ observations in which $\\vec{x}_i = (x_{i1}, x_{i2}, ..., x_{ip})$ is a $(1 \\times p)$ row vector and $y_i \\in \\{-1, 1\\}$ is a scalar. To keep things less messy, for notation purposes $x_i = \\vec{x}_i.$ Each feature vector $x_i$ has an associated $y_i$, and as well as this, we have a set of $L$ weak learners (usually decision stumps) $\\{k_1, k_2, ..., k_L\\}$ in which an individual classifer takes in a feature vector $x_i$ and outputs either a value of -1 or 1 such that $k_j(x_i) \\in \\{-1, 1\\}$ for each observation $i.$ After $(m-1)$ boosting iterations, the AdaBoost classifier is a weighted sum of each of our weak leaners such that:\n",
    "\n",
    "$$C_{(m-1)}(x_i) = a_{1}k_{1}(x_i) + ... + a_{m-1}k_{m-1}(x_i)$$\n",
    "\n",
    "If we were to add tree number $m$ (i.e. perform our $m^{th}$ boosting iteration), we then have another decision stump $k_m$ and another weight $a_m$ in which we then obtain:\n",
    "\n",
    "$$C_{m}(x_i) = C_{(m-1)}(x_i) + a_{m}k_m(x_i)$$\n",
    "\n",
    "It now becoms a case of seeing which weight $k_m$ is the most suitable new decision stump and what weight $a_m$ this should stump should have. To do this, we define a 'total error' function $E$ of $C_m$ as [\"the sum of its exponential loss on each data point\"](https://en.wikipedia.org/wiki/AdaBoost#Derivation), such that:\n",
    "\n",
    "$$E = \\sum_{i=1}^N e^{-y_iC_m(x_i)}$$\n",
    "\n",
    "Since $C_{m}(x_i) = C_{(m-1)}(x_i) + a_{m}k_m(x_i)$:\n",
    "\n",
    "$$E = \\sum_{i=1}^N e^{-y_i[C_{(m-1)}(x_i) + a_{m}k_m(x_i)]}$$\n",
    "\n",
    "By exponent laws that $e^{a+b} = e^a e^b$, we obtain:\n",
    "\n",
    "$$E = \\sum_{i=1}^N e^{-y_iC_{(m-1)}(x_i)} e^{-y_{i}a_{m}k_m(x_i)}$$\n",
    "\n",
    "Introducing $w_i^{[m]} = e^{-y_i C_{(m-1)}(x_i)}$ for $m > 1$ and $w_i^{[m]} = 1$ for $m = 1$, our total error $E$ becomes: \n",
    "\n",
    "$$E = \\sum_{i=1}^N w_i^{[m]} e^{-y_i a_m k_m(x_i)}$$\n",
    "\n",
    "Since $w_i^{[m]} = 1$ for $m = 1$, if we are in the first iteration of AdaBoost, the loss function for the first tree is given as:\n",
    "\n",
    "$$E = \\sum_{i=1}^N e^{-y_i a_1 k_1(x_i)}$$\n",
    "\n",
    "Ultimately, for correct classifications made by $k_m$, we know that $k_{m}(x_i) = y_i$, hence $y_i \\cdot k_{m}(x_i) = 1$ since $y_i \\in \\{-1, 1\\}$. For incorrect classifications where $k_{m}(x_i) \\neq y_i$, we know that $y_i \\cdot k_{m}(x_i) = -1.$ The $a_m$ term will scale this 1 or -1 value appropriately but what matters is that $E$ is larger for incorrectly classified samples and lower for correctly classified observartions. \n",
    "\n",
    "---\n",
    "\n",
    "To create an initial decision stump, we search over all of our predictor space (i.e. the set of possible values for $X_1, X_2, ... X_k$, asuuming a total of $k$ independent variables) to find the predictor $X_j; j \\in [1, k]$ and cutoff point $X_j = s$ such that by placing values where $X_j < s$ in one region and placing values $X_j \\geq s$ in another region allows us to do the best job possible in classifying the samples. This is ultimately quantified through measures such as [Gini Impurity](https://towardsdatascience.com/gini-impurity-measure-dbd3878ead33). Once we have created this stump, our current model is of the form \n",
    "\n",
    "$$C_{1}(x_i) = C_{0}(x_i) + a_{1}k_1(x_i) = 0 + a_{1}k_1(x_i) = a_{1}k_1(x_i)$$\n",
    "\n",
    "This then implies that:\n",
    "\n",
    "$$w_i^{[2]} = e^{-y_i a_1 k_1(x_i)} = e^{-y_i C_1(x_i)}$$\n",
    "\n",
    "because of the aforementioned fact that $w_i^{[m]} = e^{-y_i C_{(m-1)}(x_i)}$ for $m > 1$\n",
    "\n",
    "---\n",
    "\n",
    "For the second decision stump and onwards (i.e $m > 1$) we obtain a total error loss function given as:\n",
    "\n",
    "$$E = \\sum_{i=1}^N  e^{-y_i C_{(m-1)}(x_i)} e^{-y_i a_m k_m(x_i)} = \\sum_{i=1}^N w_i^{[m]} e^{-y_i a_m k_m(x_i)}$$\n",
    "\n",
    "\n",
    "Splitting the summation into correctly classified and incorrectly classified samples, it becomes apparent that:\n",
    "\n",
    "$$E = \\sum_{y_i = k_{m}(x_i)} w_{i}^{[m]}e^{-a_m} + \\sum_{y_i \\neq k_{m}(x_i)} w_{i}^{[m]}e^{a_m}$$\n",
    "\n",
    "to find the optimal weight $a_m$ that minimises the total error $E$, we take a derivative of our exponential loss function $E$ with respect to $a_m$ and set it equal to zero. The fact that [this exponential loss function is convex](https://yuan-du.com/post/2020-12-13-loss-functions/decision-theory/) ensures that we obtain a minimum value for $a_m$. Taking the derivative:\n",
    "\n",
    "$$\\frac{dE}{da_m} = - \\sum_{y_i = k_m(x_i)} w_i^{[m]}e^{-a_m} + \\sum_{y_i \\neq k_m(x_i)} w_i^{[m]}e^{a_m}$$\n",
    "\n",
    "Setting $\\frac{dE}{da_m} = 0$ (to minimise $a_m$), we get:\n",
    "\n",
    "$$- \\sum_{y_i = k_m(x_i)} w_i^{[m]}e^{-a_m} + \\sum_{y_i \\neq k_m(x_i)} w_i^{[m]}e^{a_m} = 0$$\n",
    "\n",
    "Since $a_m$ has no $i$ subscript and thus does not depend on the index $i$:\n",
    "\n",
    "$$e^{-a_m} \\sum_{y_i = k_m(x_i)} w_i^{[m]} = e^{a_m} \\sum_{y_i \\neq k_m(x_i)} w_i^{[m]}$$\n",
    "\n",
    "Since we wish to solve for something that is currently in the exponent, we log both sides such that:\n",
    "\n",
    "$$-a_m + ln(\\sum_{y_i = k_{m}(x_i)} w_{i}^{[m]}) = a_m + ln(\\sum_{y_i \\neq k_{m}(x_i)} w_{i}^{[m]})$$\n",
    "\n",
    "Moving terms to the other side of the equals sign:\n",
    "\n",
    "$$2a_m = ln(\\sum_{y_i = k_{m}(x_i)} w_{i}^{[m]}) - ln(\\sum_{y_i \\neq k_{m}(x_i)} w_{i}^{[m]})$$\n",
    "\n",
    "Using the log law that $ln(a) - ln(b) = ln\\big(\\frac{a}{b}\\big)$ and dividing by two on both sides:\n",
    "\n",
    "$$a_m = \\frac{1}{2}ln\\big(\\frac{\\sum_{y_i = k_{m}(x_i)} w_{i}^{[m]}}{\\sum_{y_i \\neq k_{m}(x_i)} w_{i}^{[m]}}\\big)$$\n",
    "\n",
    "Setting $W_{\\text{correctly classified}} = \\sum_{y_i = k_{m}(x_i)} w_{i}^{[m]}$ and $W_{\\text{incorrectly classified}} = \\sum_{y_i \\neq k_{m}(x_i)} w_{i}^{[m]}$ it becomes apparent that:\n",
    "\n",
    "$$a_m = \\frac{1}{2}ln\\big(\\frac{W_{\\text{correctly classified}}}{W_{\\text{incorrectly classified}}}\\big)$$\n",
    "\n",
    "Since the total sum of the weights $W_{\\text{total}} = W_{\\text{correctly classified}} + W_{\\text{incorrectly classified}}$ we see that:\n",
    "\n",
    "$$a_m = \\frac{1}{2} ln\\big(\\frac{W_{\\text{total}} -  W_{\\text{incorrectly classified}}}{W_{\\text{incorrectly classified}}}\\big)$$\n",
    "\n",
    "Setting $\\epsilon_m = \\frac{\\sum_{y_i \\neq k_{m}(x_i)} w_i^{[m]}}{\\sum_{i=1}^N w_{i}^{[m]}} = \\frac{W_{\\text{incorrectly classified}}}{W_{total}}$ it becomes clear that:\n",
    "\n",
    "$$a_m = \\frac{1}{2} ln\\big(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\big)$$\n",
    "\n",
    "which mirrors a negative logit function scaled by 0.5.\n",
    "\n",
    "It is here that we are then able to use this $a_m$ to improve our model $C_{m}(x_i) = C_{(m-1)}(x_i) + a_{m}k_m(x_i)$\n",
    "\n",
    "---\n",
    "\n",
    "Hence, AdaBoost is about four main steps\n",
    "\n",
    "1. Find the classifier (most of the time it is a decision stump) $k_m$ that minimises the total weighted error $\\sum_{y_i \\neq k_m(x_i)} w_i ^ {[m]} = W_{\\text{incorrectly classified}}$\n",
    "2. Calculate the error rate $\\epsilon_m = \\frac{W_{\\text{incorrectly classified}}}{W_{\\text total}}$\n",
    "3. Calculate $a_m = \\frac{1}{2} ln\\big(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\big)$ \n",
    "4. Use this to improve the boosted classifier $C_{m}(x_i) = C_{(m-1)}(x_i) + a_{m}k_m(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b899b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849352a",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "**Bank Dataset (not real):**\n",
    "\n",
    "Contains the follwing (in order):\n",
    "- Credit score\n",
    "- Nationality\n",
    "- Sex\n",
    "- How long they have been at the bank\n",
    "- Balance of the customer at this point in time\n",
    "- The number of products they have (i.e. having a credit cards as well as a loan means that `NumOfProducts`=2)\n",
    "- One if they have a credit card otherwise zero\n",
    "- One if they are an active member else zero\n",
    "- The estimated salary of the customer\n",
    "- **Dependent variable:** One if the person left the bank else zero\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c5b8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          502    France  Female   42       8  159660.80              3   \n",
       "2          645     Spain    Male   44       8  113755.78              2   \n",
       "3          376   Germany  Female   29       4  115046.74              4   \n",
       "4          653   Germany    Male   58       1  132602.88              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  \n",
       "1          1               0        113931.57       1  \n",
       "2          1               0        149756.71       1  \n",
       "3          1               0        119346.88       1  \n",
       "4          1               0          5097.67       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('churn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9a5c2",
   "metadata": {},
   "source": [
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2be216c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01346d6f",
   "metadata": {},
   "source": [
    "## Split into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e9bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = df.drop('Exited', axis=1)\n",
    "response_vector = df['Exited']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, response_vector, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7745f4f",
   "metadata": {},
   "source": [
    "## Construct the pipeline with preprocessing and model building steps\n",
    "\n",
    "- Assuming an already clean dataset (cleaning techniques and EDA not included in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a076d7a7",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "In a pipeline:\n",
    "\n",
    "- Apply one hot encoding to the `Geography` and `Gender` column\n",
    "- Build the AdaBoost classifier model and perform hyperparameter tuning using a randomised search of a subset of the hyperparameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22a688",
   "metadata": {},
   "source": [
    "#### Construct all steps in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61180bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one hot encoding to our sequence of things to do\n",
    "categorical_preprocessing = Pipeline(steps=[('ohe', OneHotEncoder())])\n",
    "\n",
    "# Apply one hot encoding to the 'Geography' and 'Gender' columns\n",
    "column_trans = ColumnTransformer(transformers=[\n",
    "    ('categorical_preprocessing', categorical_preprocessing, ['Geography', 'Gender'])\n",
    "])\n",
    "\n",
    "# Add the model building step to the pipeline\n",
    "# Since order is important here, the model building step came last\n",
    "adaboost_pipeline = Pipeline(steps=[\n",
    "    ('preprocess_data', column_trans), \n",
    "    ('model', AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cae44f",
   "metadata": {},
   "source": [
    "#### Construct parameter grid and fit pipeline to perform hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e4c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__n_estimators': np.arange(50, 100+1), \n",
    "    'model__learning_rate': np.linspace(0.01, 1, 10), \n",
    "    'model__algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(adaboost_pipeline, param_grid, cv=10, n_iter=25, scoring='f1').fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal Hyperparameters (according to RandomizedSearchCV): {search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee465ee8",
   "metadata": {},
   "source": [
    "## Create final model using the hyperparameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43920188",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = search.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63eedbd",
   "metadata": {},
   "source": [
    "## Make a prediction\n",
    "\n",
    "- Credit score of 700\n",
    "- From France\n",
    "- Male\n",
    "- 36 years old\n",
    "- Tenure = 8\n",
    "- Balance of 150593.19\n",
    "- NumOfProducts = 2\n",
    "- Has a credit card\n",
    "- Is an active member\n",
    "- Estimated salary of 75025.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = pd.DataFrame(data=[[700, 'France', 'Male', 36, 8, 150593.19, 2, 1, 1, 75025.27]], columns=X_train.columns)\n",
    "\n",
    "print(f'Prediction = {final_model.predict(person)}')\n",
    "print('I.E they will not churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804cec8",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880160ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Null Accuracy = {y_test.value_counts(normalize=True).values[0] * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = final_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, yhat, labels=final_model.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=final_model.classes_)\n",
    "display.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3445553",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
